# ğŸ¯ DataTchek v2.0 - Style Dataiku DSS

## ğŸ“‹ RÃ©sumÃ© des AmÃ©liorations

Transformation de DataTchek pour adopter une approche **Dataiku DSS** :

### âŒ Ce qui a Ã©tÃ© RETIRÃ‰
- âœ… Validation spÃ©cifique tÃ©lÃ©phones (format +225)
- âœ… Validation spÃ©cifique emails
- âœ… Validation spÃ©cifique comptes bancaires BCEAO
- âœ… Validation spÃ©cifique devises FCFA

### âœ… Ce qui a Ã©tÃ© AJOUTÃ‰
- âœ… **Validation sÃ©mantique intelligente** (type attendu vs type rÃ©el)
- âœ… **Nommage intelligent** basÃ© sur le fichier source
- âœ… **Nettoyage automatique** (style Dataiku Prepare)
- âœ… **Gestion donnÃ©es professionnelle**
- âœ… **Recommandations priorisÃ©es** (HAUTE/MOYENNE/BASSE)

---

## ğŸ§  1. Validation SÃ©mantique (CÅ“ur du SystÃ¨me)

### Concept

Au lieu de valider des formats spÃ©cifiques (tÃ©lÃ©phone, email), DataTchek analyse maintenant :

```
Nom de colonne â†’ Type attendu
     â†“
Contenu rÃ©el â†’ Type dÃ©tectÃ©
     â†“
Comparaison â†’ Score de conformitÃ©
```

### Exemples Concrets

| Nom Colonne | Type Attendu | Type DÃ©tectÃ© | ConformitÃ© | Analyse |
|-------------|--------------|--------------|------------|---------|
| `age` | numeric | numeric | 100% | âœ… Parfait |
| `date_naissance` | date | text | 45% | âš ï¸ ProblÃ¨me format |
| `montant` | numeric | numeric | 98% | âœ… Bon (quelques nulls) |
| `statut` | categorical | categorical | 100% | âœ… Parfait |
| `id_client` | identifier | numeric | 100% | âœ… Parfait |

### Code

```python
# column_detector.py
def infer_expected_type(column_name: str) -> str:
    """
    Devine le type attendu basÃ© sur le nom
    """
    name_lower = column_name.lower()
    
    # RÃ¨gles sÃ©mantiques
    if any(kw in name_lower for kw in ['age', 'montant', 'prix', 'total']):
        return 'numeric'
    
    if any(kw in name_lower for kw in ['date', 'dt', 'naissance']):
        return 'date'
    
    if any(kw in name_lower for kw in ['id', 'code', 'ref']):
        return 'identifier'
    
    # ... etc
    return 'text'

def detect_actual_type(series: pd.Series) -> str:
    """
    DÃ©tecte le type rÃ©el en analysant le contenu
    """
    # Analyse Ã©chantillon
    # Retourne: numeric, date, boolean, categorical, text, etc.
```

---

## ğŸ“› 2. Nommage Intelligent des Fichiers

### Principe

Tous les fichiers gÃ©nÃ©rÃ©s (nettoyÃ©s, rapports, analyses) portent un nom **basÃ© sur le fichier source** + timestamp.

### Module `file_naming.py`

```python
from file_naming import FileNamingManager

# Fichier source
manager = FileNamingManager("donnees_clients_2024.csv")

# GÃ©nÃ©ration automatique des noms
manager.generate_cleaned_filename()
# â†’ "donnees_clients_2024_cleaned_20260130_151234.csv"

manager.generate_report_filename('pdf')
# â†’ "rapport_donnees_clients_2024_20260130_151234.pdf"

manager.generate_analysis_filename()
# â†’ "analyse_donnees_clients_2024_20260130_151234.json"
```

### Avantages

âœ… **TraÃ§abilitÃ©** : Lien clair avec le fichier source  
âœ… **Pas de collision** : Timestamp garantit unicitÃ©  
âœ… **Organisation** : Facile de retrouver les fichiers liÃ©s  
âœ… **Professionnel** : Nommage cohÃ©rent  

### Exemples Complets

| Fichier Source | Fichier NettoyÃ© | Rapport PDF |
|----------------|-----------------|-------------|
| `clients_janvier.csv` | `clients_janvier_cleaned_20260130_143022.csv` | `rapport_clients_janvier_20260130_143022.pdf` |
| `Ventes 2024.xlsx` | `ventes_2024_cleaned_20260130_150145.csv` | `rapport_ventes_2024_20260130_150145.pdf` |
| `DATA-EXPORT.csv` | `data_export_cleaned_20260130_152233.csv` | `rapport_data_export_20260130_152233.pdf` |

---

## ğŸ§¹ 3. Nettoyage Automatique (Style Dataiku Prepare)

### Module `data_cleaner.py`

Classe `DataCleaner` qui permet de :

```python
from data_cleaner import DataCleaner

# CrÃ©er nettoyeur
cleaner = DataCleaner(df, filename="donnees_clients_2024.csv")

# Nettoyage complet automatique
cleaner.auto_clean(aggressive=False)

# RÃ©cupÃ©rer le DataFrame nettoyÃ©
df_cleaned = cleaner.get_cleaned_dataframe()

# RÃ©cupÃ©rer le rapport de nettoyage
report = cleaner.get_cleaning_report()
```

### OpÃ©rations Disponibles

| OpÃ©ration | Description | Mode |
|-----------|-------------|------|
| `remove_empty_columns()` | Supprime colonnes 100% vides | Auto |
| `remove_high_missing_columns(threshold=0.8)` | Supprime colonnes >80% manquants | Auto |
| `remove_duplicates()` | Supprime lignes dupliquÃ©es | Auto |
| `remove_constant_columns()` | Supprime colonnes avec 1 seule valeur | Agressif |
| `fill_missing_numeric(strategy='median')` | Impute valeurs manquantes numÃ©riques | Auto |
| `fill_missing_categorical(strategy='mode')` | Impute valeurs manquantes catÃ©gorielles | Auto |
| `standardize_column_names()` | Standardise noms (snake_case) | Auto |
| `convert_to_numeric(columns)` | Convertit en numÃ©rique | Agressif |
| `convert_to_datetime(columns)` | Convertit en datetime | Agressif |
| `remove_whitespace()` | Supprime espaces inutiles | Auto |

### Exemple Complet

```python
# Fichier source
df = pd.read_csv("donnees_clients_2024.csv")

# CrÃ©er nettoyeur
cleaner = DataCleaner(df, "donnees_clients_2024.csv")

# Nettoyage automatique
cleaner.auto_clean(aggressive=True)

# RÃ©cupÃ©rer rÃ©sultats
df_cleaned = cleaner.get_cleaned_dataframe()
report = cleaner.get_cleaning_report()

# Sauvegarder avec nom intelligent
cleaned_filename = cleaner.generate_cleaned_filename()
df_cleaned.to_csv(cleaned_filename, index=False)

print(report)
# {
#   'source_file': 'donnees_clients_2024.csv',
#   'original_shape': (1000, 25),
#   'cleaned_shape': (987, 20),
#   'rows_removed': 13,
#   'columns_removed': 5,
#   'operations': [
#       {'operation': 'remove_empty_columns', 'columns': ['col1', 'col2'], 'count': 2},
#       {'operation': 'remove_duplicates', 'rows_removed': 13},
#       ...
#   ]
# }
```

---

## ğŸ“Š 4. Recommandations PriorisÃ©es

### SystÃ¨me de PrioritÃ©s

Les recommandations sont maintenant **priorisÃ©es** comme dans Dataiku :

```python
recommendations = [
    {
        "priority": "HAUTE",        # ğŸ”´
        "category": "Doublons",
        "message": "âš ï¸ 150 doublons dÃ©tectÃ©s (15% des donnÃ©es)",
        "action": "Supprimer ou fusionner les lignes dupliquÃ©es"
    },
    {
        "priority": "MOYENNE",      # ğŸŸ 
        "category": "CohÃ©rence sÃ©mantique",
        "message": "âš¡ Colonne 'age' : ConformitÃ© 65%",
        "action": "VÃ©rifier et nettoyer les 350 valeurs suspectes"
    },
    {
        "priority": "BASSE",        # â„¹ï¸
        "category": "Optimisation",
        "message": "â„¹ï¸ Colonne 'statut' : TrÃ¨s faible diversitÃ© (3 valeurs)",
        "action": "Convertir en type catÃ©goriel pour optimiser"
    }
]
```

### Dans le Rapport PDF

```
5. RECOMMANDATIONS

ğŸ”´ PrioritÃ© HAUTE :
â€¢ âš ï¸ 150 doublons dÃ©tectÃ©s (15% des donnÃ©es)
  â†’ Supprimer ou fusionner les lignes dupliquÃ©es

â€¢ âš ï¸ Colonne 'email' : 85% de valeurs manquantes
  â†’ Supprimer la colonne ou imputer les valeurs

ğŸŸ  PrioritÃ© MOYENNE :
â€¢ âš¡ Colonne 'age' : Type 'text' au lieu de 'numeric'
  â†’ Convertir en numÃ©rique et corriger les 45 valeurs invalides

â„¹ï¸ PrioritÃ© BASSE :
â€¢ Colonne 'statut' : TrÃ¨s faible diversitÃ©
  â†’ Convertir en catÃ©goriel pour optimiser mÃ©moire
```

---

## ğŸ“ˆ 5. Calcul du Score de QualitÃ©

### Nouvelle Formule (style Dataiku)

```python
Score Global = 100 points

- Doublons (max -15 points)
  â””â”€ PÃ©nalitÃ© = min(15, % doublons Ã— 1.5)

- Valeurs manquantes (max -25 points)
  â””â”€ PÃ©nalitÃ© = min(25, % manquants Ã— 1.2)

- IncohÃ©rence sÃ©mantique (max -35 points)
  â””â”€ Pour chaque colonne :
      â€¢ ConformitÃ© < 50% : -3 points
      â€¢ ConformitÃ© 50-70% : -2 points
      â€¢ ConformitÃ© 70-90% : -1 point

- Faible cardinalitÃ© sur clÃ©s (max -10 points)
  â””â”€ Si colonne avec 'id'/'code' a <80% valeurs uniques : -2 points

+ Bonus qualitÃ© excellente (max +10 points)
  â””â”€ Si 0 doublons ET <5% manquants : +10
```

### Exemple Calcul

```
Dataset : 1000 lignes, 20 colonnes

- Doublons : 2% â†’ -3 points
- Manquants : 15% â†’ -18 points
- SÃ©mantique : 3 colonnes avec problÃ¨mes â†’ -7 points
- CardinalitÃ© : OK â†’ 0 point
- Bonus : non applicable

Score final : 100 - 3 - 18 - 7 = 72/100 ğŸŸ¡ MOYEN
```

---

## ğŸ”„ 6. Workflow Complet

### Ã‰tape par Ã‰tape

```
1. UPLOAD
   User: Upload "donnees_clients_2024.csv"
   â†“
   App: Charge fichier + crÃ©e FileNamingManager

2. ANALYSE AUTOMATIQUE
   App: Analyse qualitÃ© via validate_dataframe()
   â†“
   RÃ©sultats:
   - Score global : 72/100
   - Doublons : 2%
   - Manquants : 15%
   - SÃ©mantique : 3 colonnes incohÃ©rentes

3. AFFICHAGE RÃ‰SULTATS
   App: Affiche score + mÃ©triques + recommandations
   â†“
   User: Consulte recommandations priorisÃ©es

4. NETTOYAGE AUTO (optionnel)
   User: Clic "Lancer nettoyage automatique"
   â†“
   App: DataCleaner.auto_clean(aggressive=True)
   â†“
   RÃ©sultats nettoyage:
   - 13 lignes supprimÃ©es (doublons)
   - 5 colonnes supprimÃ©es (>80% manquants)
   - Noms colonnes standardisÃ©s

5. TÃ‰LÃ‰CHARGEMENTS
   User: TÃ©lÃ©charge fichiers
   â†“
   Fichiers gÃ©nÃ©rÃ©s:
   - donnees_clients_2024_cleaned_20260130_151234.csv
   - rapport_donnees_clients_2024_20260130_151234.pdf
```

---

## ğŸ“¦ 7. Structure des Fichiers

### Arborescence Projet

```
datatcheck/
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ validators.py           # â† Validation sÃ©mantique (NOUVEAU)
â”‚   â”œâ”€â”€ column_detector.py      # DÃ©tection types
â”‚   â”œâ”€â”€ data_cleaner.py         # â† Nettoyage auto (NOUVEAU)
â”‚   â”œâ”€â”€ file_naming.py          # â† Nommage intelligent (NOUVEAU)
â”‚   â”œâ”€â”€ pdf_generator.py        # GÃ©nÃ©ration PDF
â”‚   â””â”€â”€ visualizations.py       # Graphiques
â”œâ”€â”€ app.py                       # â† Application Streamlit (MISE Ã€ JOUR)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### Fichiers Principaux ModifiÃ©s

1. **`validators.py`** (v2.0)
   - âŒ SupprimÃ© : CIValidators (tÃ©lÃ©phone, email, IBAN)
   - âœ… AjoutÃ© : Validation sÃ©mantique type vs nom
   - âœ… AjoutÃ© : Recommandations priorisÃ©es
   - âœ… AjoutÃ© : MÃ©triques qualitÃ© dÃ©taillÃ©es

2. **`file_naming.py`** (NOUVEAU)
   - Classe FileNamingManager
   - GÃ©nÃ©ration noms intelligents
   - Standardisation noms datasets

3. **`data_cleaner.py`** (NOUVEAU)
   - Classe DataCleaner
   - 10+ opÃ©rations nettoyage
   - Mode auto vs agressif
   - Rapport dÃ©taillÃ©

4. **`app.py`** (v2.0)
   - Interface style Dataiku
   - Workflow complet
   - Nettoyage intÃ©grÃ©
   - TÃ©lÃ©chargements multiples

---

## ğŸ¯ 8. Utilisation

### Installation

```bash
pip install streamlit pandas numpy plotly reportlab
```

### Lancement

```bash
streamlit run app.py
```

### Exemple d'Utilisation

```python
from utils.validators import validate_dataframe
from utils.data_cleaner import DataCleaner
from utils.file_naming import FileNamingManager

# 1. Charger donnÃ©es
df = pd.read_csv("donnees_clients_2024.csv")

# 2. CrÃ©er gestionnaire nommage
naming = FileNamingManager("donnees_clients_2024.csv")

# 3. Analyser qualitÃ©
results = validate_dataframe(df, filename="donnees_clients_2024.csv")

print(f"Score : {results['quality_score']}/100")
print(f"Doublons : {results['duplicates']['count']}")
print(f"Manquants : {results['missing_values']['percentage']}%")

# 4. Nettoyer (si nÃ©cessaire)
if results['quality_score'] < 80:
    cleaner = DataCleaner(df, "donnees_clients_2024.csv")
    cleaner.auto_clean(aggressive=True)
    
    df_cleaned = cleaner.get_cleaned_dataframe()
    report = cleaner.get_cleaning_report()
    
    # 5. Sauvegarder avec nom intelligent
    cleaned_filename = naming.generate_cleaned_filename()
    df_cleaned.to_csv(cleaned_filename, index=False)
    
    print(f"Fichier nettoyÃ© : {cleaned_filename}")
```

---

## âœ… 9. Checklist Changements

### Ce qui FONCTIONNE maintenant

- âœ… Validation sÃ©mantique (type vs nom colonne)
- âœ… Nommage intelligent fichiers (basÃ© sur source)
- âœ… Nettoyage automatique (10+ opÃ©rations)
- âœ… Recommandations priorisÃ©es (HAUTE/MOYENNE/BASSE)
- âœ… Rapport PDF avec nom intelligent
- âœ… Score qualitÃ© basÃ© sur sÃ©mantique
- âœ… Gestion donnÃ©es style Dataiku DSS

### Ce qui a Ã©tÃ© RETIRÃ‰

- âŒ Validation tÃ©lÃ©phone CI (+225)
- âŒ Validation email spÃ©cifique
- âŒ Validation IBAN BCEAO
- âŒ Validation devise FCFA

### Pourquoi ces retraits ?

**Raison 1 : Focus sÃ©mantique**
- Plus gÃ©nÃ©rique et adaptable
- Fonctionne pour tous pays/contextes
- BasÃ© sur la logique mÃ©tier (nom â†’ type)

**Raison 2 : Style Dataiku**
- Dataiku ne valide pas des formats spÃ©cifiques
- Il analyse la cohÃ©rence type vs nom
- Approche plus professionnelle

**Raison 3 : ExtensibilitÃ©**
- Facile d'ajouter nouveaux types sÃ©mantiques
- Pas besoin de coder validateurs pour chaque format
- RÃ¨gles sÃ©mantiques dans fichier config

---

## ğŸš€ 10. Prochaines Ã‰tapes

### Pour amÃ©liorer encore

1. **Profiling avancÃ©** (comme Dataiku Statistics)
   - Distribution des valeurs
   - DÃ©tection outliers
   - CorrÃ©lations entre colonnes

2. **Suggestions de transformations**
   - "Colonne X devrait Ãªtre splittÃ©e en 2"
   - "Colonne Y devrait Ãªtre en majuscules"

3. **DÃ©tection de patterns mÃ©tier**
   - DÃ©tection emails automatique (mÃªme sans "email" dans nom)
   - DÃ©tection tÃ©lÃ©phones automatique
   - DÃ©tection codes postaux

4. **Export vers Dataiku**
   - Export au format Dataiku DSS
   - GÃ©nÃ©ration de recipes

---

## ğŸ“ Support

Pour toute question :
- **Version** : 2.0 - Style Dataiku DSS
- **Date** : 30 Janvier 2026
- **Auteur** : HABIB KOFFI

---

**ğŸ‰ DataTchek v2.0 est prÃªt !**

Focus sur la validation sÃ©mantique intelligente, le nommage professionnel et le nettoyage automatique, exactement comme Dataiku DSS.